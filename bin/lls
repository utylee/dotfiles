#!/usr/bin/env python3
"""
lls: tiny llama-server launcher (Ollama-ish UX)

- Reads ~/.config/lls/config.jsonc (JSONC: // and /* */ comments allowed)
- Resolves params = defaults + platform profile + model_rule profiles + model_rule override
- start: launches llama-server, streams logs to screen + file (unless --detach)
- stop/status/list/show-profile: helpers for tmux/fzf workflows

Python 3.8+ compatible.
"""

import argparse
import json
import os
import re
import socket
import signal
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# -------------------------
# Paths / env
# -------------------------
HOME = Path.home()
DEFAULT_CONFIG_PATH = HOME / ".config" / "lls" / "config.jsonc"


def env_path(name: str, default: Path) -> Path:
    v = os.environ.get(name, "")
    return Path(v) if v else default


def default_models_dir() -> Path:
    # You told me your real path is ~/temp/llm_models on WSL2.
    # Keep fallback to ~/llm_models for portability.
    return env_path("LLS_MODELS_DIR", HOME / "temp" / "llm_models")


def default_llama_bin() -> Path:
    # Prefer explicit env var; fallback to ~/bin/llama-server
    # return env_path("LLS_LLAMA_BIN", HOME / "bin" / "llama-server")
    return env_path("LLS_LLAMA_BIN", "llama-server")


def default_state_dir() -> Path:
    return env_path("LLS_STATE_DIR", HOME / ".local" / "state" / "lls")


def default_logs_dir() -> Path:
    return env_path("LLS_LOGS_DIR", default_state_dir() / "logs")

def build_llama_cmd(llama_bin: Path, model_path: Path, params: dict) -> list:
    return [
        str(llama_bin),
        "--host", str(params["host"]),
        "--port", str(params["port"]),
        "--model", str(model_path),
        "--ctx-size", str(params["ctx"]),
        "--threads", str(params["threads"]),
        "--batch-size", str(params["batch"]),
        "--ubatch-size", str(params["ubatch"]),
        "--n-gpu-layers", str(params["gpu_layers"]),
    ]


def format_cmd(cmd: list) -> str:
    # 공백/특수문자 안전하게 보기 좋게
    import shlex
    return " ".join(shlex.quote(str(x)) for x in cmd)

def run_show_profile(args):
    home = Path.home()
    cfg_path = Path(os.environ.get("LLS_CONFIG", str(home / ".config/lls/config.jsonc")))
    cfg = load_jsonc(cfg_path)

    models_dir = Path(os.environ.get("LLS_MODELS_DIR", str(home / "llm_models")))
    llama_bin  = Path(os.environ.get("LLS_LLAMA_BIN", str(home / "bin/llama-server")))

    platform = args.platform or os.environ.get("LLS_PLATFORM", "wsl")
    model_path = pick_model(models_dir, args.model)

    params = resolve_params(cfg, platform, model_path.name)

    # 옵션 override도 반영 (start랑 동일)
    if args.host:
        params["host"] = args.host
    if args.port is not None:
        params["port"] = int(args.port)

    cmd = build_llama_cmd(llama_bin, model_path, params)

    print(f"Model: {model_path.name}")
    print(f"Platform: {platform}")
    print("")
    for k in ("host","port","ctx","threads","batch","ubatch","gpu_layers"):
        if k in params:
            print(f"{k}: {params[k]}")
    print("\nCommand:")
    print(format_cmd(cmd))

def run_info(args):
    home = Path.home()
    state_dir = Path(os.environ.get("LLS_STATE_DIR", str(home / ".local/state/lls")))
    port = args.port

    statefile = state_dir / f"llama_{port}.json"
    pidfile = state_dir / f"llama_{port}.pid"

    if not statefile.exists():
        # fallback: pidfile만 있으면 pid 정도만
        if pidfile.exists():
            pid = pidfile.read_text().strip()
            print(f"running? pidfile exists (pid={pid}) port={port} (no state json)")
        else:
            print("stopped")
        return

    import json
    s = json.loads(statefile.read_text(encoding="utf-8"))
    print(f"status: running pid={s.get('pid')} port={s.get('port')} host={s.get('host')}")
    print(f"model: {s.get('model')}")
    print(f"log: {s.get('logfile')}")
    print("cmd:")
    print(format_cmd(s.get("cmd", [])))





# -------------------------
# JSONC parsing
# -------------------------
def strip_jsonc(text: str) -> str:
    # Remove // line comments (not perfect for URLs inside strings, but OK for this config style)
    text = re.sub(r"//.*?$", "", text, flags=re.M)
    # Remove /* ... */ block comments
    text = re.sub(r"/\*.*?\*/", "", text, flags=re.S)
    return text


def load_jsonc(path: Path) -> Dict[str, Any]:
    raw = path.read_text(encoding="utf-8")
    return json.loads(strip_jsonc(raw))


def deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(a)
    for k, v in (b or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = deep_merge(out[k], v)
        else:
            out[k] = v
    return out


# -------------------------
# Model selection / rule matching
# -------------------------
def pick_model(models_dir: Path, token: str) -> Path:
    """
    token may be:
      - exact filename
      - substring of filename (case-insensitive), first match in sorted order
    """
    p = models_dir / token
    if p.exists() and p.is_file():
        return p

    token_l = token.lower()
    cands = sorted(models_dir.glob("*.gguf"))
    for c in cands:
        if token_l in c.name.lower():
            return c
    raise FileNotFoundError(f"Model not found: {token} (dir={models_dir})")


def rule_matches(rule_match: str, model_name: str) -> bool:
    """
    Your config comment says "부분매칭/정규식 느낌".
    So we try regex search first; if pattern is invalid, fallback to substring.
    Matching is case-insensitive.
    """
    if not rule_match:
        return False
    model_lc = model_name.lower()
    pat = str(rule_match)

    try:
        return re.search(pat, model_lc, flags=re.I) is not None
    except re.error:
        return pat.lower() in model_lc


def resolve_params(
    cfg: Dict[str, Any],
    platform: str,
    model_name: str,
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    Resolve final params and return (params, debug).
    Resolution order:
      1) defaults
      2) platform profile (e.g., "wsl" / "mac")
      3) first matching model_rule:
           - apply use_profiles in listed order
           - apply override
    """
    defaults = cfg.get("defaults", {}) or {}
    profiles = cfg.get("profiles", {}) or {}
    rules = cfg.get("model_rules", []) or []

    params: Dict[str, Any] = dict(defaults)
    applied: List[str] = []
    matched_rule: Optional[Dict[str, Any]] = None

    # platform
    if platform and platform in profiles and isinstance(profiles[platform], dict):
        params = deep_merge(params, profiles[platform])
        applied.append(platform)

    # first-match rule
    for rule in rules:
        m = (rule or {}).get("match", "")
        if not m:
            continue
        if rule_matches(str(m), model_name):
            matched_rule = rule
            break

    if matched_rule:
        for prof in (matched_rule.get("use_profiles", []) or []):
            if prof in profiles and isinstance(profiles[prof], dict):
                params = deep_merge(params, profiles[prof])
                applied.append(prof)
        override = matched_rule.get("override", {}) or {}
        if isinstance(override, dict):
            params = deep_merge(params, override)

    debug = {
        "platform": platform,
        "model": model_name,
        "matched_rule": (matched_rule.get("match") if matched_rule else None),
        "applied_profiles": applied,
        "rule": matched_rule,
    }
    return params, debug


# -------------------------
# Commands
# -------------------------
def run_list(args: argparse.Namespace) -> None:
    models_dir = env_path("LLS_MODELS_DIR", default_models_dir())
    for p in sorted(models_dir.glob("*.gguf")):
        print(p.name)


def _pidfile_for_port(state_dir: Path, port: int) -> Path:
    return state_dir / f"llama_{port}.pid"


# ----------------------------
# Extra lightweight cache state (for fast status/info)
# ~/.cache/lls/<port>.json
# ----------------------------
def default_cache_state_dir() -> Path:
    return Path.home() / ".cache" / "lls"


def _cache_statefile_for_port(port: int) -> Path:
    return default_cache_state_dir() / f"{port}.json"


def write_state(port: int, pid: int, model: str, cmdline: str) -> None:
    d = default_cache_state_dir()
    d.mkdir(parents=True, exist_ok=True)
    p = d / f"{port}.json"
    data = {"pid": pid, "port": port, "model": model, "cmd": cmdline}
    p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


def read_state(port: int) -> Optional[Dict[str, Any]]:
    p = _cache_statefile_for_port(port)
    if not p.exists():
        return None
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return None


def remove_state(port: int) -> None:
    try:
        _cache_statefile_for_port(port).unlink()
    except FileNotFoundError:
        pass


def _statefile_for_port(state_dir: Path, port: int) -> Path:
    # managed detailed state json
    return state_dir / f"llama_{port}.json"


def parse_overrides(tokens: List[str]) -> Tuple[Dict[str, Any], bool]:
    """
    tokens: argparse.REMAINDER; allow -d/--detach anywhere.
    Returns: (overrides_dict, detach_flag_from_tokens)
    """
    detach = False
    filtered: List[str] = []
    for t in tokens:
        if t in ("-d", "--detach"):
            detach = True
        else:
            filtered.append(t)

    out: Dict[str, Any] = {}
    i = 0
    while i < len(filtered):
        t = filtered[i]
        if not t.startswith("--"):
            raise SystemExit(f"override: unexpected token {t}")
        key = t[2:].replace("-", "_")
        # bool flag
        if i + 1 >= len(filtered) or filtered[i + 1].startswith("--"):
            out[key] = True
            i += 1
            continue
        val = filtered[i + 1]
        if re.fullmatch(r"-?\d+", val):
            v: Any = int(val)
        else:
            if val.lower() in ("true", "false"):
                v = (val.lower() == "true")
            else:
                try:
                    v = float(val)
                except ValueError:
                    v = val
        out[key] = v
        i += 2

    return out, detach

def is_pid_alive(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except Exception:
        return False


def port_open(host: str, port: int, timeout: float = 0.5) -> bool:
    try:
        with socket.create_connection((host, port), timeout=timeout):
            return True
    except Exception:
        return False

def run_status(args: argparse.Namespace) -> None:
    # Managed pidfile first; fallback to port check (external)
    state_dir = env_path("LLS_STATE_DIR", default_state_dir())
    port = int(args.port)
    pidfile = _pidfile_for_port(state_dir, port)
    statefile = _statefile_for_port(state_dir, port)

    if pidfile.exists():
        try:
            pid = int(pidfile.read_text().strip())
        except Exception:
            print("stale pidfile (unreadable)")
            return

        if is_pid_alive(pid):
            cached = read_state(port)
            if not cached and statefile.exists():
                try:
                    cached = json.loads(statefile.read_text(encoding="utf-8"))
                except Exception:
                    cached = None

            if cached:
                print("running")
                print(f"  pid    : {pid}")
                print(f"  port   : {port}")
                print(f"  model  : {cached.get('model')}")
            else:
                print(f"running pid={pid} port={port} (managed)")

            return

        # pidfile exists but pid dead
        if not port_open("127.0.0.1", port):
            pidfile.unlink(missing_ok=True)
            statefile.unlink(missing_ok=True)
            remove_state(port)
            print("stopped (cleaned stale pid/state)")
            return

        print(f"running (external process on port {port}); stale pidfile exists")
        return

    if port_open("127.0.0.1", port):
        print(f"running (external process on port {port})")
        print("  model  : unknown (not managed by lls)")
        return

    print("stopped")

def run_stop(args: argparse.Namespace) -> None:
    state_dir = env_path("LLS_STATE_DIR", default_state_dir())
    port = int(args.port)
    pidfile = _pidfile_for_port(state_dir, port)
    statefile = _statefile_for_port(state_dir, port)

    if not pidfile.exists():
        if port_open("127.0.0.1", port):
            print(f"[lls] port {port} is open but no pidfile (external). Stop manually.")
        else:
            print(f"[lls] not running (no pidfile for port {port})")
        return

    try:
        pid = int(pidfile.read_text().strip())
    except Exception:
        pidfile.unlink(missing_ok=True)
        statefile.unlink(missing_ok=True)
        remove_state(port)
        print("[lls] stale pidfile cleaned")
        return

    if not is_pid_alive(pid):
        pidfile.unlink(missing_ok=True)
        statefile.unlink(missing_ok=True)
        remove_state(port)
        print("[lls] pid not found; cleaned pid/state")
        return

    try:
        os.kill(pid, signal.SIGINT)
    except Exception:
        pass

    t0 = time.time()
    while time.time() - t0 < 3.0:
        if not is_pid_alive(pid):
            pidfile.unlink(missing_ok=True)
            statefile.unlink(missing_ok=True)
            remove_state(port)
            print("[lls] stopped")
            return
        time.sleep(0.1)

    try:
        os.kill(pid, signal.SIGTERM)
    except Exception:
        pass

    pidfile.unlink(missing_ok=True)
    statefile.unlink(missing_ok=True)
    remove_state(port)
    print("[lls] stopped (forced)")

def run_show_profile(args: argparse.Namespace) -> None:
    cfg_path = Path(args.config)
    cfg = load_jsonc(cfg_path)

    models_dir = env_path("LLS_MODELS_DIR", default_models_dir())

    # Accept substring just like start: resolve to a concrete filename for clarity
    try:
        model_path = pick_model(models_dir, args.model)
        model_name = model_path.name
    except Exception:
        # If models_dir isn't available in this context, fall back to the raw token
        model_name = args.model

    platform = args.platform or os.environ.get("LLS_PLATFORM", "wsl")
    params, dbg = resolve_params(cfg, platform, model_name)

    print(f"Model: {dbg['model']}")
    print(f"Platform: {dbg['platform']}")
    print(f"Matched rule: {dbg['matched_rule']}")
    print(f"Applied profiles: {', '.join(dbg['applied_profiles']) if dbg['applied_profiles'] else '(none)'}")
    print("")

    keys = ["host", "port", "ctx", "threads", "batch", "ubatch", "gpu_layers"]
    for k in keys:
        if k in params:
            print(f"{k}: {params[k]}")

    extra = {k: v for k, v in params.items() if k not in set(keys)}
    if extra:
        print("\nExtra:")
        for k in sorted(extra.keys()):
            print(f"{k}: {extra[k]}")


def run_start(args: argparse.Namespace) -> None:
    cfg_path = env_path("LLS_CONFIG", DEFAULT_CONFIG_PATH)
    cfg = load_jsonc(cfg_path)

    models_dir = env_path("LLS_MODELS_DIR", default_models_dir())
    llama_bin = env_path("LLS_LLAMA_BIN", default_llama_bin())

    platform = os.environ.get("LLS_PLATFORM", "wsl")  # mac / wsl

    model_path = pick_model(models_dir, args.model)
    params, dbg = resolve_params(cfg, platform, model_path.name)

    # Apply direct CLI overrides first (host/port)
    if getattr(args, "host", None):
        params["host"] = args.host
    if getattr(args, "port", None) is not None:
        params["port"] = int(args.port)

    # Temporary overrides (argparse.REMAINDER), also allow -d/--detach anywhere
    detach_from_tokens = False
    if getattr(args, "overrides", None):
        ov, detach_from_tokens = parse_overrides(list(args.overrides))
        params.update(ov)

    detach = bool(getattr(args, "detach", False)) or detach_from_tokens

    port = int(params.get("port", 8080))
    host = str(params.get("host", "0.0.0.0"))

    state_dir = env_path("LLS_STATE_DIR", default_state_dir())
    logs_dir = env_path("LLS_LOGS_DIR", default_logs_dir())
    state_dir.mkdir(parents=True, exist_ok=True)
    logs_dir.mkdir(parents=True, exist_ok=True)

    pidfile = _pidfile_for_port(state_dir, port)
    statefile = _statefile_for_port(state_dir, port)
    logfile = logs_dir / f"{model_path.name}.log"

    # already running?
    if pidfile.exists():
        try:
            pid = int(pidfile.read_text().strip())
            os.kill(pid, 0)
            print(f"[lls] Already running (pid={pid}) on port {port}. Stop first: lls stop --port {port}", file=sys.stderr)
            sys.exit(2)
        except Exception:
            pidfile.unlink(missing_ok=True)
            statefile.unlink(missing_ok=True)
            remove_state(port)

    cmd = build_llama_cmd(llama_bin, model_path, params)

    print("[lls] Starting llama-server:")
    print("      " + format_cmd(cmd))
    print(f"[lls] log file: {logfile}")

    state = {
        "pid": None,
        "port": port,
        "host": host,
        "model": model_path.name,
        "model_path": str(model_path),
        "logfile": str(logfile),
        "cmd": cmd,
        "started_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "platform": platform,
        "resolved": params,
        "dbg": dbg,
    }

    def _cleanup_files() -> None:
        pidfile.unlink(missing_ok=True)
        statefile.unlink(missing_ok=True)
        remove_state(port)

    def _write_state(pid: int) -> None:
        pidfile.write_text(str(pid), encoding="utf-8")
        state["pid"] = pid
        statefile.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")
        write_state(port, pid, model_path.name, format_cmd(cmd))

    if detach:
        with open(logfile, "ab", buffering=0) as f:
            p = subprocess.Popen(cmd, stdout=f, stderr=subprocess.STDOUT)

        if p.poll() is not None:
            _cleanup_files()
            print(f"[lls] failed to start (rc={p.returncode}). See log: {logfile}", file=sys.stderr)
            sys.exit(p.returncode or 1)

        _write_state(p.pid)
        print(f"[lls] started (pid={p.pid})")
        return

    # Foreground mode: stream logs safely, don't crash on non-utf8
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=0)

    if p.poll() is not None:
        _cleanup_files()
        print(f"[lls] failed to start (rc={p.returncode})", file=sys.stderr)
        sys.exit(p.returncode or 1)

    _write_state(p.pid)

    try:
        assert p.stdout is not None
        with open(logfile, "ab", buffering=0) as f:
            while True:
                chunk = p.stdout.readline()
                if not chunk:
                    break
                f.write(chunk)
                sys.stdout.write(chunk.decode("utf-8", errors="replace"))
                sys.stdout.flush()
    except KeyboardInterrupt:
        print("\n[lls] Ctrl+C received. Stopping...", file=sys.stderr)
        try:
            p.send_signal(signal.SIGINT)
        except Exception:
            pass
    except Exception as e:
        print(f"\n[lls] ERROR while streaming logs: {e}", file=sys.stderr)
        try:
            p.terminate()
        except Exception:
            pass
        _cleanup_files()
        raise

    rc = p.wait()
    _cleanup_files()
    print(f"[lls] exited rc={rc}")
    sys.exit(rc)

def main() -> None:
    ap = argparse.ArgumentParser(prog="lls")
    sub = ap.add_subparsers(dest="cmd", required=True)

    sp = sub.add_parser("start", help="start llama-server with a model")
    sp.add_argument("model", help="model filename or substring")
    sp.add_argument("-d", "--detach", action="store_true", help="run in background (no live logs)")
    sp.add_argument("--host", help="override host for this run")
    sp.add_argument("--port", type=int, help="override port for this run")
    sp.add_argument("overrides", nargs=argparse.REMAINDER, help="temporary overrides like --ctx 8192 --batch 512 (also accepts -d)")
    sp.set_defaults(func=run_start)

    sp = sub.add_parser("stop", help="stop llama-server")
    sp.add_argument("--port", type=int, default=int(os.environ.get("LLAMA_PORT", "8080")))
    sp.set_defaults(func=run_stop)

    sp = sub.add_parser("status", help="show status")
    sp.add_argument("--port", type=int, default=int(os.environ.get("LLAMA_PORT", "8080")))
    sp.set_defaults(func=run_status)

    sp = sub.add_parser("list", help="list gguf models")
    sp.set_defaults(func=run_list)

    sp = sub.add_parser("show-profile", help="show resolved params for a model (from config)")
    sp.add_argument("model", help="model filename (or substring, like start)")
    sp.add_argument("--platform", choices=["wsl", "mac"], help="platform profile (or set $LLS_PLATFORM)")
    sp.add_argument("--config", default=str(DEFAULT_CONFIG_PATH), help="path to config.jsonc")
    sp.add_argument("--host", help="override host for preview")
    sp.add_argument("--port", type=int, help="override port for preview")
    sp.set_defaults(func=run_show_profile)

    sp = sub.add_parser("info", help="show current running model/port/log/cmd")
    sp.add_argument("--port", type=int, default=int(os.environ.get("LLAMA_PORT", "8080")))
    sp.set_defaults(func=run_info)




    args = ap.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
